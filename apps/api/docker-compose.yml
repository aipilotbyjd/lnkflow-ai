# LinkFlow API - Docker Compose
# Works standalone (start infra first) or via root include
#
# Standalone:
#   cd infra && docker-compose up -d    # Start Postgres + Redis first
#   docker-compose up -d                # Start API + Queue
#   docker-compose --profile full up -d # Start all (including scheduler)
#
# Full stack (from project root):
#   docker-compose up -d                # Everything via include

services:
  # ============================================
  # Laravel API
  # ============================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: linkflow-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    env_file:
      - .env.docker
    environment:
      DB_HOST: linkflow-postgres
      REDIS_HOST: linkflow-redis
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      LINKFLOW_ENGINE_SECRET: ${LINKFLOW_SECRET}
      ENGINE_PARTITION_COUNT: ${ENGINE_PARTITION_COUNT:-16}
      REDIS_QUEUE: workflows-default
    volumes:
      - .:/var/www/html
    networks:
      - linkflow
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/v1/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Queue Worker
  # ============================================
  queue:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: linkflow-queue
    restart: unless-stopped
    env_file:
      - .env.docker
    environment:
      DB_HOST: linkflow-postgres
      REDIS_HOST: linkflow-redis
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      LINKFLOW_ENGINE_SECRET: ${LINKFLOW_SECRET}
      ENGINE_PARTITION_COUNT: ${ENGINE_PARTITION_COUNT:-16}
    volumes:
      - .:/var/www/html
    command: php artisan queue:work redis --queue=workflows-high,workflows-default,workflows-low,default --sleep=3 --tries=3 --max-time=3600 --max-jobs=1000 --verbose
    healthcheck:
      test: [ "CMD-SHELL", "ps aux | grep -v grep | grep 'queue:work' || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      api:
        condition: service_healthy
    networks:
      - linkflow
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Scheduler (Cron) - Optional
  # ============================================
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: linkflow-scheduler
    restart: unless-stopped
    env_file:
      - .env.docker
    environment:
      DB_HOST: linkflow-postgres
      REDIS_HOST: linkflow-redis
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      LINKFLOW_ENGINE_SECRET: ${LINKFLOW_SECRET}
      ENGINE_PARTITION_COUNT: ${ENGINE_PARTITION_COUNT:-16}
    volumes:
      - .:/var/www/html
    command: >
      sh -c "while true; do php artisan schedule:run --verbose; sleep 60; done"
    healthcheck:
      test: [ "CMD-SHELL", "pgrep -f 'schedule:run|sleep' || exit 1" ]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      api:
        condition: service_healthy
    networks:
      - linkflow
    profiles:
      - full
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

networks:
  linkflow:
    name: linkflow
    driver: bridge
